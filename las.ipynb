{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMYgx6Q1Ly_f"
      },
      "source": [
        "# Start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Od6BeoRAjzKL"
      },
      "outputs": [],
      "source": [
        "DOWNLOAD_DATA = 1\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "CON_TRAIN = 1\n",
        "\n",
        "DEBUG = 0\n",
        "\n",
        "SUBMIT_TO_KAGGLE = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBrQ8OHjW6qC"
      },
      "source": [
        "# Libraries and Initial Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTGPr98x0yjO",
        "outputId": "89c7fdc9-fee2-4e8e-8aba-d856f81f9bf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True 3.7.13 (default, Apr 24 2022, 01:04:09) \n",
            "[GCC 7.5.0]\n",
            "Cuda = True with num_workers = 4\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import Levenshtein as lev\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn.utils as utils\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import random\n",
        "import datetime\n",
        "from torch.utils import data\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pdb\n",
        "cuda = torch.cuda.is_available()\n",
        "\n",
        "print(cuda, sys.version)\n",
        "\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "num_workers = 4 if cuda else 0\n",
        "print(\"Cuda = \"+str(cuda)+\" with num_workers = \"+str(num_workers))\n",
        "np.random.seed(11785)\n",
        "torch.manual_seed(11785)\n",
        "\n",
        "# The labels of the dataset contain letters in LETTER_LIST.\n",
        "# You should use this to convert the letters to the corresponding indices\n",
        "# and train your model with numerical labels.\n",
        "LETTER_LIST = ['<sos>', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', \\\n",
        "         'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', \"'\", ' ', '<eos>']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oscfzpYiKQ5I"
      },
      "outputs": [],
      "source": [
        "if DEBUG:\n",
        "    device = \"cpu\"\n",
        "    BATCH_SIZE = 2\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xc7E_BuxZ1g"
      },
      "outputs": [],
      "source": [
        "def create_dictionaries(letter_list):\n",
        "    '''\n",
        "    Create dictionaries for letter2index and index2letter transformations\n",
        "    based on LETTER_LIST\n",
        "\n",
        "    Args:\n",
        "        letter_list: LETTER_LIST\n",
        "\n",
        "    Return:\n",
        "        letter2index: Dictionary mapping from letters to indices\n",
        "        index2letter: Dictionary mapping from indices to letters\n",
        "    '''\n",
        "    letter2index = dict()\n",
        "    index2letter = dict()\n",
        "    \n",
        "    letter2index = {ele:ind for ind,ele in enumerate(letter_list)}\n",
        "    index2letter = {ind:ele for ind,ele in enumerate(letter_list)}\n",
        "\n",
        "    return letter2index, index2letter\n",
        "\n",
        "def transform_index_to_letter(batch_indices):  #(B,Tmax)\n",
        "    '''\n",
        "    Transforms numerical index input to string output by converting each index \n",
        "    to its corresponding letter from LETTER_LIST\n",
        "\n",
        "    Args:\n",
        "        batch_indices: List of indices from LETTER_LIST with the shape of (N, )\n",
        "    \n",
        "    Return:\n",
        "        transcripts: List of converted string transcripts. This would be a list with a length of N\n",
        "    '''\n",
        "    transcripts = []\n",
        "    # return transcripts without <eos>\n",
        "    stop_ind = letter2index['<eos>']\n",
        "    for indices in batch_indices:\n",
        "        transcript = \"\" # string of one transcript\n",
        "        for idx in indices:\n",
        "            if (idx == stop_ind) or ((idx>=len(LETTER_LIST))or idx<=0):\n",
        "                break\n",
        "            else:\n",
        "                transcript += index2letter[idx]\n",
        "        transcripts.append(transcript)\n",
        "    return transcripts\n",
        "letter2index, index2letter = create_dictionaries(LETTER_LIST)"
      ]
    },
    
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WuigASA6U7s"
      },
      "outputs": [],
      "source": [
        "if DOWNLOAD_DATA:\n",
        "\n",
        "    !kaggle competitions download -c 11-785-s22-hw4p2\n",
        "\n",
        "    !unzip 11-785-s22-hw4p2.zip\n",
        "\n",
        "    !ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5ioyn6ldQB9"
      },
      "source": [
        "# Dataset and Dataloading\n",
        "\n",
        "You will need to implement the Dataset class by your own. You can implement it similar to HW3P2. However, you are welcomed to do it your own way if it is more comfortable or efficient.\n",
        "\n",
        "Note that you need to use LETTER_LIST to convert the transcript into numerical labels for the model.\n",
        "\n",
        "\n",
        "Example of raw transcript:\n",
        "\n",
        "    ['<sos>', 'N', 'O', 'R', 'T', 'H', 'A', 'N', 'G', 'E', 'R', ' ','A', 'B', 'B', 'E', 'Y', '<eos>']\n",
        "\n",
        "Example of converted transcript ready to process for the model:\n",
        "\n",
        "    [0, 14, 15, 18, 20, 8, 1, 14, 7, 5, 18, 28, 1, 2, 2, 5, 25, 29]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksXYcsjRQ5-6"
      },
      "outputs": [],
      "source": [
        "# cepstral normalization\n",
        "# x tensor\n",
        "def normalize(x):\n",
        "    x_hat = x - x.mean(dim=0)\n",
        "    return x_hat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SndiVRVqBMa"
      },
      "outputs": [],
      "source": [
        "# simple data Libri\n",
        "class LibriSimple(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, data_path, partition= \"train\"): # You can use partition to specify train or dev\n",
        "\n",
        "        self.X_path = data_path +  \"/\" + partition + \".npy\"\n",
        "        self.Y_path = data_path +  \"/\" + partition + \"_transcripts.npy\"\n",
        "        \n",
        "        # load all data here \n",
        "        xx = np.load(self.X_path, allow_pickle=True) # 10000 obj of np arra\n",
        "        self.X = [torch.from_numpy(x_arr) for x_arr in xx]\n",
        "        Y = np.load(self.Y_path, allow_pickle=True) # Y[0] is a list\n",
        "        self.Yy=[]\n",
        "\n",
        "        #  ~~~~ keep <eos>\n",
        "        for Y_ls in Y:\n",
        "            self.Yy.append(torch.LongTensor([letter2index[yy] for yy in Y_ls[1:]])) #convert the transcript into numerical labels for the model\n",
        "\n",
        "        assert(len(self.X) == len(self.Yy))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.Yy)\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        return normalize(self.X[ind]),  self.Yy[ind]\n",
        "    \n",
        "    def collate_fn(self, batch):\n",
        "\n",
        "        batch_x = [x for x,y in batch]\n",
        "        batch_y = [y for x,y in batch]\n",
        "\n",
        "        batch_x_pad = pad_sequence(batch_x, batch_first=True) # TODO: pad the sequence with pad_sequence (already imported)\n",
        "        lengths_x = [len(x) for x in batch_x]# TODO: Get original lengths of the sequence before padding\n",
        "\n",
        "        batch_y_pad = pad_sequence(batch_y, batch_first=True)# TODO: pad the sequence with pad_sequence (already imported)\n",
        "        lengths_y = [len(y) for y in batch_y]# TODO: Get original lengths of the sequence before padding\n",
        "\n",
        "        return batch_x_pad, batch_y_pad, torch.tensor(lengths_x), torch.tensor(lengths_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mjRVesgD7A7"
      },
      "outputs": [],
      "source": [
        "class LibriSamples(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, data_path, partition= \"train\"): # You can use partition to specify train or dev\n",
        "\n",
        "        self.X_dir = data_path + \"/\" + partition + \"/mfcc/\" # mfcc directory path\n",
        "        self.Y_dir = data_path + \"/\" + partition +\"/transcript/\" # transcript path\n",
        "\n",
        "        self.X_files = os.listdir(self.X_dir)   # list files in the mfcc directory\n",
        "        self.Y_files = os.listdir(self.Y_dir)   # list files in the transcript directory\n",
        "\n",
        "        assert(len(self.X_files) == len(self.Y_files))\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X_files)\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "    \n",
        "        X_path = self.X_dir + self.X_files[ind]\n",
        "        Y_path = self.Y_dir + self.Y_files[ind]\n",
        "        \n",
        "        # Y is raw transcript\n",
        "        X = torch.from_numpy(np.load(X_path)) # Load the mfcc npy file at the specified index ind in the directory\n",
        "        Y = np.load(Y_path) # Load the corresponding transcripts\n",
        "        #  ~~~~ keep <eos>\n",
        "        Yy = torch.LongTensor([letter2index[yy] for yy in Y[1:]]) #convert the transcript into numerical labels for the model\n",
        "\n",
        "        return normalize(X), Yy\n",
        "    \n",
        "    # pad x/y to the same length within one batch\n",
        "    def collate_fn(self, batch):\n",
        "\n",
        "        batch_x = [x for x,y in batch]\n",
        "        batch_y = [y for x,y in batch]\n",
        "\n",
        "        batch_x_pad = pad_sequence(batch_x, batch_first=True) # TODO: pad the sequence with pad_sequence (already imported)\n",
        "        lengths_x = [len(x) for x in batch_x]# TODO: Get original lengths of the sequence before padding\n",
        "\n",
        "        batch_y_pad = pad_sequence(batch_y, batch_first=True)# TODO: pad the sequence with pad_sequence (already imported)\n",
        "        lengths_y = [len(y) for y in batch_y]# TODO: Get original lengths of the sequence before padding\n",
        "\n",
        "        return batch_x_pad, batch_y_pad, torch.tensor(lengths_x), torch.tensor(lengths_y)\n",
        "\n",
        "# You can either try to combine test data in the previous class or write a new Dataset class for test data\n",
        "class LibriSamplesTest(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, data_path, partition = \"test\", test_order = None): # test_order is the csv similar to what you used in hw1\n",
        "        # TODO: Load the npy files from test_order.csv and append into a list\n",
        "        # You can load the files here or save the paths here and load inside __getitem__ like the previous class\n",
        "        \n",
        "        self.X_dir = data_path + \"/\" + partition + \"/mfcc/\"\n",
        "        self.X_names = os.listdir(self.X_dir)\n",
        "        \n",
        "        if test_order:\n",
        "            self.X_names = list(pd.read_csv(test_order).file)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X_names)\n",
        "    \n",
        "    def __getitem__(self, ind):\n",
        "        X_path = self.X_dir + self.X_names[ind]\n",
        "        X = torch.from_numpy(np.load(X_path))\n",
        "        \n",
        "        return normalize(X)\n",
        "    \n",
        "    def collate_fn(self, batch):\n",
        "        batch_x = [x for x in batch]\n",
        "        batch_x_pad = pad_sequence(batch_x, batch_first=True) # TODO: pad the sequence with pad_sequence (already imported)\n",
        "        lengths_x = [len(x) for x in batch_x]# TODO: Get original lengths of the sequence before padding\n",
        "\n",
        "        return batch_x_pad, torch.tensor(lengths_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mzoYfTKu14s",
        "outputId": "9375bc20-8c95-44b5-dc29-c400d4f9ca9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size:  128\n",
            "Train dataset samples = 28539, batches = 223\n",
            "Val dataset samples = 2703, batches = 22\n",
            "Test dataset samples = 2620, batches = 21\n"
          ]
        }
      ],
      "source": [
        "batch_size = BATCH_SIZE\n",
        "root = '/content/hw4p2_student_data/hw4p2_student_data'\n",
        "\n",
        "train_data = LibriSamples(root, partition=\"train\")\n",
        "val_data = LibriSamples(root, partition=\"dev\")\n",
        "test_data = LibriSamplesTest(root, partition=\"test\", test_order=root+\"/test/test_order.csv\")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=train_data.collate_fn, pin_memory=True)\n",
        "# TODO: Define the val loader. Remember to pass in a parameter (function) for the collate_fn argument \n",
        "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False, collate_fn=val_data.collate_fn, pin_memory=True)\n",
        "# TODO: Define the test loader. Remember to pass in a parameter (function) for the collate_fn argument \n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, collate_fn=test_data.collate_fn, pin_memory=True)\n",
        "\n",
        "print(\"Batch size: \", batch_size)\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9FwVZ9I2da0"
      },
      "outputs": [],
      "source": [
        "# test code for checking shapes\n",
        "if DEBUG:\n",
        "    for data in train_loader:\n",
        "        x, y, lx, ly = data\n",
        "        print(x.shape, y.shape, lx.shape, len(ly))\n",
        "        print(y[0]) # desired \n",
        "        break\n",
        "\n",
        "    for data in val_loader:\n",
        "        x, y, lx, ly = data\n",
        "        print(x.shape, y.shape, lx.shape, len(ly))\n",
        "        print(y[0]) # desired \n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I--VjKlEhwi8"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RjtOwo36vcC"
      },
      "outputs": [],
      "source": [
        "# fixed mask every time step\n",
        "class LockedDropout(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x, dropout=0.5):\n",
        "        if not self.training or not dropout:\n",
        "            return x\n",
        "        batch_size, seq_length, feat_size = x.size()\n",
        "        m = x.data.new(batch_size, 1, feat_size).bernoulli_(1 - dropout)\n",
        "        #mask = Variable(m, requires_grad=False) / (1 - dropout)\n",
        "        mask = m / (1 - dropout)\n",
        "        mask = mask.expand_as(x)\n",
        "        return mask * x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0KEvzl-ApKf"
      },
      "source": [
        "## Listen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfpIMUDzCvT3"
      },
      "outputs": [],
      "source": [
        "class pBLSTM(nn.Module):\n",
        "    '''\n",
        "    Pyramidal BiLSTM\n",
        "    Read paper and understand the concepts and then write your implementation here.\n",
        "\n",
        "    At each step,\n",
        "    1. Pad your input if it is packed\n",
        "    2. Truncate the input length dimension by concatenating feature dimension\n",
        "        (i) How should  you deal with odd/even length input? \n",
        "        (ii) How should you deal with input length array (x_lens) after truncating the input?\n",
        "    3. Pack your input\n",
        "    4. Pass it into LSTM layer\n",
        "\n",
        "    To make our implementation modular, we pass 1 layer at a time.\n",
        "    '''\n",
        "    def __init__(self, input_dim, hidden_dim, dropouth=0.5):\n",
        "        super(pBLSTM, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        #self.dropout = dropout\n",
        "        #self.dropouti = dropouti\n",
        "        self.dropouth = dropouth\n",
        "        ### drop out for one layer LSTM, only modify the hidden layers\n",
        "\n",
        "        self.blstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
        "        self.lockdrop = LockedDropout()\n",
        "\n",
        "    def forward(self, x): # packed in, packed out\n",
        "        # x=(B,seq_len,input_size)=(B,T,dim) # dim=2*hidden_dim\n",
        "        x_padded, x_len = pad_packed_sequence(x, batch_first=True)\n",
        "        x_len = x_len.to(device)\n",
        "        # truncate, make T even\n",
        "        x_padded = x_padded[:, :(x_padded.size(1) // 2) * 2, :] # (B, T, dim)\n",
        "        # reshape to (B, T/2, dim*2)\n",
        "        x_reshaped = x_padded.reshape(x_padded.size(0), x_padded.size(1) // 2, x_padded.size(2) * 2)\n",
        "        x_len = x_len // 2\n",
        "\n",
        "        x_packed = pack_padded_sequence(x_reshaped, lengths=x_len.cpu(), batch_first=True, enforce_sorted=False) #(B,T/2,dim*2)\n",
        "        \n",
        "        # Outputs: output, (h_n, c_n)\n",
        "        out_packed, _ = self.blstm(x_packed)\n",
        "        # input = (B,T/2,h_dim*4)\n",
        "        # output = (B,T/2,h_dim*2)\n",
        "\n",
        "        ## unpack and drop and pack again\n",
        "        out, out_len = pad_packed_sequence(out_packed, batch_first= True)\n",
        "        out_len = out_len.to(device)\n",
        "        out = self.lockdrop(out, self.dropouth)  \n",
        "        out_packed = pack_padded_sequence(out, lengths=out_len.cpu(), batch_first=True, enforce_sorted=False) \n",
        "        \n",
        "        return out_packed\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEmFI9It54mk"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    '''\n",
        "    Encoder takes the utterances as inputs and returns the key, value and unpacked_x_len.\n",
        "\n",
        "    '''\n",
        "    def __init__(self, input_dim, encoder_hidden_dim, key_value_size=128, dropouti=0):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        #self.encoder_hidden_dim = encoder_hidden_dim\n",
        "\n",
        "        #self.dropout = dropout\n",
        "        self.dropouti = dropouti\n",
        "        #self.dropouth = dropouth\n",
        "\n",
        "        # The first LSTM layer at the bottom\n",
        "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=encoder_hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
        "        \n",
        "        # Define the blocks of pBLSTMs\n",
        "        # Dimensions should be chosen carefully\n",
        "        # Hint: Bidirectionality, truncation...\n",
        "        self.pBLSTMs = nn.Sequential(\n",
        "            pBLSTM(encoder_hidden_dim*4, encoder_hidden_dim),\n",
        "            pBLSTM(encoder_hidden_dim*4, encoder_hidden_dim),\n",
        "            pBLSTM(encoder_hidden_dim*4, encoder_hidden_dim, dropouth=0) #  no drop out in last layer\n",
        "            # Optional: dropout\n",
        "            # ...\n",
        "        )\n",
        "\n",
        "        self.lockdrop = LockedDropout()\n",
        "        \n",
        "        # The linear transformations for producing Key and Value for attention\n",
        "        # Hint: Dimensions when bidirectional lstm? \n",
        "        self.key_network = nn.Linear(encoder_hidden_dim*2, key_value_size)\n",
        "        self.value_network = nn.Linear(encoder_hidden_dim*2, key_value_size)\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        # LSTM layer taks h_state and c_state in that order.[h, c]\n",
        "        weight = next(self.parameters()).data\n",
        "        return [(weight.new(1,bsz, self.encoder_hidden_dim).zero_(),\n",
        "                    weight.new(1,bsz, self.encoder_hidden_dim).zero_())]\n",
        "\n",
        "    def forward(self, x, x_len, hidden = None):\n",
        "        \"\"\"\n",
        "        1. Pack your input and pass it through the first LSTM layer (no truncation)\n",
        "        2. Pass it through the pyramidal LSTM layer\n",
        "        3. Pad your input back to (B, T, *) or (T, B, *) shape\n",
        "        4. Output Key, Value, and truncated input lens\n",
        "\n",
        "        Key and value could be\n",
        "            (i) Concatenated hidden vectors from all time steps (key == value).\n",
        "            (ii) Linear projections of the output from the last pBLSTM network.\n",
        "                If you choose this way, you can use the final output of\n",
        "                your pBLSTM network.\n",
        "        \"\"\"\n",
        "\n",
        "        ## dropout to input before packing\n",
        "        x = self.lockdrop(x, self.dropouti)\n",
        "\n",
        "        ## do not need \n",
        "        # if hidden is None:\n",
        "        #     hidden = self.init_hidden(x.size(0))\n",
        "\n",
        "        # pack input\n",
        "        x_packed = pack_padded_sequence(x, lengths=x_len.cpu(), enforce_sorted=False, batch_first=True) #(B,T,vocab_size)\n",
        "        # output from first LSTM layer\n",
        "        out1_packed, _ = self.lstm(x_packed) #(B,T,dim*2)\n",
        "\n",
        "        ## can do unpack and locked dropout between these two\n",
        "\n",
        "        # pass to pblstm\n",
        "        out_packed = self.pBLSTMs(out1_packed) #(B,T/8,dim*2)\n",
        "        # pad back\n",
        "        out, lh  = pad_packed_sequence(out_packed, batch_first = True)\n",
        "        # get key and value\n",
        "        key = self.key_network(out)\n",
        "        value = self.value_network(out)\n",
        "\n",
        "        return key, value, lh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7TEBpEF6VFh"
      },
      "outputs": [],
      "source": [
        "# encoder = Encoder(input_dim=13,encoder_hidden_dim=256)\n",
        "# # Try out your encoder on a tiny input before moving to the next step...\n",
        "# print(encoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSRofd3OArtw"
      },
      "source": [
        "## Attend\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqu-MUM8TjUO"
      },
      "outputs": [],
      "source": [
        "# def plot_attention(attention):\n",
        "#     # utility function for debugging\n",
        "#     plt.clf()\n",
        "#     sns.heatmap(attention, cmap='GnBu')\n",
        "#     plt.show()\n",
        "\n",
        "# dot-production attention\n",
        "class Attention(nn.Module):\n",
        "    '''\n",
        "    Attention is calculated using key and value from encoder and query from decoder.\n",
        "    1. Dot-product attention\n",
        "        energy = bmm(key, query) \n",
        "        # Optional: Scaled dot-product by normalizing with sqrt key dimension\n",
        "        # Check \"attention is all you need\" Section 3.2.1\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "        # Optional: dropout\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, query, key, value, mask):\n",
        "        \"\"\"\n",
        "        input:\n",
        "            key: (batch_size, seq_len, d_k)\n",
        "            value: (batch_size, seq_len, d_v)\n",
        "            query: (batch_size, d_q)\n",
        "\n",
        "        * Hint: d_k == d_v == d_q is often true if you use linear projections\n",
        "        return:\n",
        "            context: (batch_size, key_val_dim)\n",
        "            key_val_dim = d_k = d_v\n",
        "        \n",
        "        \"\"\"\n",
        "        # A = Q @ K.T\n",
        "        # energy = torch.bmm(key, query.unsqueeze(2)).squeeze(2)\n",
        "        energy = torch.bmm(query.unsqueeze(1), key.transpose(1, 2)).squeeze(1) #(B,T)      \n",
        "        energy.masked_fill_(mask, -1e9)\n",
        "        attention = self.softmax(energy) #(B,T)\n",
        "        context = torch.bmm(attention.unsqueeze(1),value).squeeze(1) #(B,dim)\n",
        "        \n",
        "        return context, attention\n",
        "        # we return attention weights for plotting (for debugging)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxZhOHzZAxyC"
      },
      "source": [
        "## Spell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8Ukh6KdCee-"
      },
      "outputs": [],
      "source": [
        "class EmbeddingDropout(nn.Module):\n",
        "    \"\"\"\n",
        "    Applies dropout in the embedding layer by zeroing out some elements of\n",
        "    the embedding vector. Dropout is applied to full rows of the embedding\n",
        "    matrix: we drop out entire words and not components of a word's dense\n",
        "    embedding.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_prob):\n",
        "        super().__init__()\n",
        "        self.embed_prob = embed_prob\n",
        "\n",
        "    def forward(self, emb, words):\n",
        "        if self.training and self.embed_prob != 1:\n",
        "            mask = torch.from_numpy(np.random.binomial(1, self.embed_prob, size=(emb.weight.data.shape[0])) / self.embed_prob).to(device)\n",
        "            masked_embed = mask.unsqueeze(1) * emb.weight\n",
        "        else:\n",
        "            masked_embed = emb.weight\n",
        "        return(F.embedding(words.long(), emb.weight).float())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcTC4cK95TYT"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    '''\n",
        "    As mentioned in a previous recitation, each forward call of decoder deals with just one time step.\n",
        "    Thus we use LSTMCell instead of LSTM here.\n",
        "    The output from the last LSTMCell can be used as a query for calculating attention.\n",
        "    Methods like Gumble noise and teacher forcing can also be incorporated for improving the performance.\n",
        "    '''\n",
        "    def __init__(self, vocab_size, decoder_hidden_dim, embed_dim, key_value_size=128, dropouth=0.5, dropoutemb=0.3):\n",
        "        super(Decoder, self).__init__()\n",
        "        # Hint: Be careful with the padding_idx\n",
        "\n",
        "        # provide a lookup table for 30 chrs\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim) #(30,256)\n",
        "\n",
        "        # add embedding dropout (word level dropout)\n",
        "        self.embedding_dropout = EmbeddingDropout(embed_prob=1-dropoutemb)\n",
        "\n",
        "        # The number of cells is defined based on the paper\n",
        "        # RNN is a 2 layer LSTM.\n",
        "        self.lstm1 = nn.LSTMCell(embed_dim+key_value_size, decoder_hidden_dim)\n",
        "        self.lstm2 = nn.LSTMCell(decoder_hidden_dim,key_value_size)\n",
        "    \n",
        "        self.attention = Attention()    # add layers later\n",
        "        self.vocab_size = vocab_size\n",
        "        \n",
        "        #self.character_prob = nn.Linear(key_value_size*2, vocab_size) #: d_v -> vocab_size\n",
        "        self.key_value_size = key_value_size\n",
        "\n",
        "        # enhance fc layer\n",
        "        self.character_prob = nn.Sequential(\n",
        "            nn.Linear(key_value_size * 2,\n",
        "                      key_value_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(key_value_size, vocab_size))\n",
        "        \n",
        "        # Optional: Weight-tying\n",
        "        self.character_prob.weight = self.embedding.weight\n",
        "\n",
        "        # locked dropout lol\n",
        "        # #self.dropout = dropout\n",
        "        # self.dropouti = dropouti\n",
        "        self.dropouth = dropouth\n",
        "\n",
        "\n",
        "        self.drop = nn.Dropout(self.dropouth)\n",
        "\n",
        "    def forward(self, key, value, encoder_len, y=None, mode='train'):\n",
        "        '''\n",
        "        Args:\n",
        "            key :(B, T, d_k) - Output of the Encoder (possibly from the Key projection layer)\n",
        "            value: (B, T, d_v) - Output of the Encoder (possibly from the Value projection layer)\n",
        "            y: (B, text_len) - Batch input of text with text_length\n",
        "            mode: Train or eval mode for teacher forcing\n",
        "\n",
        "            encoder_len = lx: (batch_size, )\n",
        "        Return:\n",
        "            predictions: the character perdiction probability \n",
        "        '''\n",
        "        # **********LAS: 1. decoder rnn 2. attention 3. concate and MLP\n",
        "\n",
        "\n",
        "\n",
        "        B, key_seq_max_len, key_value_size = key.shape\n",
        "\n",
        "        if mode == 'train':\n",
        "            max_len =  y.shape[1]\n",
        "            # char_embeddings = self.embedding(y) #(B,Tmax,emb_size)\n",
        "            # if using embdding drop:\n",
        "            char_embeddings = self.embedding_dropout(self.embedding,y.long()).to(device)\n",
        "        else:\n",
        "            max_len = 600\n",
        "\n",
        "        # TODO: Create the attention mask here (outside the for loop rather than inside) to aviod repetition\n",
        "        # mask padded part\n",
        "        mask = torch.arange(key_seq_max_len).unsqueeze(0) >= encoder_len.unsqueeze(1) # (1,T) >= (B,1)-> (B,Tmax)\n",
        "        mask = mask.to(device)\n",
        "        \n",
        "        predictions = []\n",
        "        # This is the first input to the decoder\n",
        "        # What should the fill_value be? <sos>\n",
        "        prediction = torch.full((B,1), fill_value=0, device=device)  # (B,1)\n",
        "        # The length of hidden_states vector should depend on the number of LSTM Cells defined in init\n",
        "        # The paper uses 2 [h_i, c_i]\n",
        "        hidden_states = [None, None] \n",
        "        # hidden_states = [(torch.zeros(batch_size, self.hidden_dim).to(device), torch.zeros(batch_size, self.hidden_dim).to(DEVICE)), \\\n",
        "        #                  (torch.zeros(batch_size, self.key_size).to(device), torch.zeros(batch_size, self.key_size).to(DEVICE))]\n",
        "        \n",
        "        \n",
        "        # TODO: Initialize the context\n",
        "        context = value[:, 0, :]    # (B,1,dim)\n",
        "\n",
        "        attention_plot = [] # this is for debugging\n",
        "\n",
        "        rate = 0.9\n",
        "        gumbel_noise = True\n",
        "\n",
        "        for i in range(max_len):\n",
        "            if mode == 'train':\n",
        "                # TODO: Implement Teacher Forcing\n",
        "                teacher_forcing_choice = np.random.choice([0,1],p=[1-rate,rate])\n",
        "                #pdb.set_trace()\n",
        "                if teacher_forcing_choice:\n",
        "                    if i == 0:\n",
        "                        # This is the first time step\n",
        "                        # Hint: How did you initialize \"prediction\" variable above? \"<sos>\"\n",
        "                        start_char = torch.zeros((B,), dtype=torch.long).fill_(0).to(device)\n",
        "                        char_embed = self.embedding(start_char)\n",
        "                    else:\n",
        "                        # Otherwise, feed the label of the **previous** time step (ground trueth)\n",
        "                        char_embed = char_embeddings[:,i-1,:]\n",
        "                else: # inclued gumble_noise\n",
        "                    if i!=0 and (gumbel_noise):\n",
        "                        char_embed = F.gumbel_softmax(prediction).mm(self.embedding.weight)\n",
        "                    else:\n",
        "                        char_embed = self.embedding(prediction.argmax(dim=-1)) #embedding of the previous prediction\n",
        "            else: # no ground truth if not train\n",
        "                 char_embed = self.embedding(prediction.argmax(dim=-1)) # embedding of the previous prediction\n",
        "\n",
        "            # step 1. decoder RNN: s_i = RNN(s_i−1,y_i−1,c_i−1)\n",
        "            # what vectors should be concatenated as a context?\n",
        "            #pdb.set_trace()\n",
        "            # y_i-1 = char_embed; c_i-1 = context\n",
        "            y_context = torch.cat([char_embed, context],dim=1) # maybe wrong (128,256) (128,128)\n",
        "            # context and hidden states of lstm 1 from the previous time step should be fed\n",
        "            hidden_states[0] = self.lstm1(y_context, hidden_states[0])# fill this out)\n",
        "\n",
        "            ## add locked dropout here (only between two layers), already at one time-step, so use normal dropout\n",
        "            out1 = self.drop(hidden_states[0][0])\n",
        "\n",
        "            # hidden states of lstm1 and hidden states of lstm2 from the previous time step should be fed\n",
        "            hidden_states[1] = self.lstm2(out1, hidden_states[1])\n",
        "            #hidden_states[1] = self.lstm2(hidden_states[0][0], hidden_states[1]) # no dropout option\n",
        "            # What then is the query? at the first time step\n",
        "            query = hidden_states[1][0] # h_1\n",
        "\n",
        "            # step 2. attention: c_i = AttentionContext(s_i,h)\n",
        "            # Compute attention from the output of the second LSTM Cell\n",
        "            context, attention = self.attention(query, key, value, mask) \n",
        "            # We store the first attention of this batch for debugging\n",
        "            attention_plot.append(attention[0].detach().cpu())\n",
        "            \n",
        "            # step 3. concate s_i and c_i, and input to MLP\n",
        "            # What should be concatenated as the output context?\n",
        "            output_context = torch.cat([query, context], dim=1) \n",
        "            prediction = self.character_prob(output_context) # (B, vocab_size)\n",
        "            # store predictions\n",
        "            predictions.append(prediction.unsqueeze(1))# char pred\n",
        "        \n",
        "        \n",
        "        # Concatenate the attention and predictions to return\n",
        "        attentions = torch.stack(attention_plot, dim=0) #(T,T)\n",
        "        predictions = torch.cat(predictions, dim=1) # (B,Tmax,vocab_size)\n",
        "\n",
        "        return predictions, attentions\n",
        "        #return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d35FEZhz5Uhx"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    '''\n",
        "    We train an end-to-end sequence to sequence model comprising of Encoder and Decoder.\n",
        "    This is simply a wrapper \"model\" for your encoder and decoder.\n",
        "    '''\n",
        "    def __init__(self, input_dim, vocab_size, encoder_hidden_dim, decoder_hidden_dim, embed_dim, key_value_size=128):\n",
        "        super(Seq2Seq,self).__init__()\n",
        "        self.encoder = Encoder(input_dim, encoder_hidden_dim)# fill this out)\n",
        "        self.decoder = Decoder(vocab_size, decoder_hidden_dim, embed_dim)# fill this out)\n",
        "\n",
        "    def forward(self, x, x_len, y=None, mode='train'):\n",
        "        key, value, encoder_len = self.encoder(x, x_len)\n",
        "        predictions, attentions = self.decoder(key, value, encoder_len, y=y, mode=mode)\n",
        "        #predictions= self.decoder(key, value, encoder_len, y=y, mode=mode)\n",
        "        return predictions, attentions\n",
        "        #return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsgiRC2G8cj-"
      },
      "source": [
        "# Model Info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9h-vz1fT09K",
        "outputId": "848f003b-abb6-4dd4-d853-b184516c82ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seq2Seq(\n",
            "  (encoder): Encoder(\n",
            "    (lstm): LSTM(13, 256, batch_first=True, bidirectional=True)\n",
            "    (pBLSTMs): Sequential(\n",
            "      (0): pBLSTM(\n",
            "        (blstm): LSTM(1024, 256, batch_first=True, bidirectional=True)\n",
            "        (lockdrop): LockedDropout()\n",
            "      )\n",
            "      (1): pBLSTM(\n",
            "        (blstm): LSTM(1024, 256, batch_first=True, bidirectional=True)\n",
            "        (lockdrop): LockedDropout()\n",
            "      )\n",
            "      (2): pBLSTM(\n",
            "        (blstm): LSTM(1024, 256, batch_first=True, bidirectional=True)\n",
            "        (lockdrop): LockedDropout()\n",
            "      )\n",
            "    )\n",
            "    (lockdrop): LockedDropout()\n",
            "    (key_network): Linear(in_features=512, out_features=128, bias=True)\n",
            "    (value_network): Linear(in_features=512, out_features=128, bias=True)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (embedding): Embedding(30, 256)\n",
            "    (embedding_dropout): EmbeddingDropout()\n",
            "    (lstm1): LSTMCell(384, 512)\n",
            "    (lstm2): LSTMCell(512, 128)\n",
            "    (attention): Attention(\n",
            "      (softmax): Softmax(dim=-1)\n",
            "    )\n",
            "    (character_prob): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
            "      (1): Tanh()\n",
            "      (2): Linear(in_features=128, out_features=30, bias=True)\n",
            "    )\n",
            "    (drop): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = Seq2Seq(input_dim=13,vocab_size=30,encoder_hidden_dim=256,decoder_hidden_dim=512,embed_dim=256)# fill this out)\n",
        "model = model.to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHMzR6fLht5n"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVdFK_EpA1ZO"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHQjxRJ8iTPy"
      },
      "outputs": [],
      "source": [
        "#  edit distance : number of character modifications needed to change the sequence to the gold sequence. \n",
        "def calc_edit_dist(preds, targets):\n",
        "    tot_dist = 0.0\n",
        "    for pred, target in zip(preds, targets):\n",
        "        dist = lev.distance(pred, target)\n",
        "#         print(\"Lev dist {}\".format(dist))\n",
        "        tot_dist += dist\n",
        "    return tot_dist/(len(preds))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIXzhQclhs98"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, criterion, optimizer, mode, epoch):\n",
        "    model.train()\n",
        "\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train❤') \n",
        "\n",
        "    running_loss = 0 # total_loss actually\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    # 0) Iterate through your data loader\n",
        "    for batch_num, (x,y,x_len,y_len) in enumerate(train_loader):\n",
        "        # 1) Send the inputs to the device\n",
        "        x, x_len, y, y_len = x.to(device), x_len.to(device), y.to(device), y_len.to(device)\n",
        "        # 2) Pass your inputs, and length of speech into the model.\n",
        "        predictions, attentions = model(x, x_len, y, mode='train') # (B, Tmax, vocab_size)\n",
        "        # 3) Generate a mask based on target length. This is to mark padded elements\n",
        "\n",
        "        # so that we can exclude them from computing loss.\n",
        "        # Ensure that the mask is on the device and is the correct shape.\n",
        "\n",
        "        mask = torch.zeros(y.size()).to(device)\n",
        "        for i in range(len(y_len)):\n",
        "          mask[i,:y_len[i]] = 1\n",
        "        #mask = mask[:,1:]\n",
        "        mask = mask.reshape(-1)\n",
        "        \n",
        "        # 4) Make sure you have the correct shape of predictions when putting into criterion\n",
        "        loss = criterion(predictions.view(-1, predictions.size(2)), y.view(-1))\n",
        "        # Use the mask you defined above to compute the average loss\n",
        "        masked_loss = torch.sum(loss*mask)\n",
        "        # Sum this masked loss and divide by sum of target lengths \n",
        "        # current_loss = masked_loss /(sum(y_len) - batch_size)\n",
        "        current_loss = masked_loss / torch.sum(mask)\n",
        "        running_loss += current_loss \n",
        "        # 5) backprop\n",
        "        optimizer.zero_grad()\n",
        "        current_loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Optional: Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2)\n",
        "\n",
        "        # print the training loss after every N batches\n",
        "        # tqdm lets you add some details so you can monitor training as you train.\n",
        "        batch_bar.set_postfix(\n",
        "            loss=\"{:.04f}\".format(float(running_loss / (batch_num + 1))),\n",
        "            lr=\"{:.08f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "        batch_bar.update() # Update tqdm bar\n",
        "    batch_bar.close() \n",
        "\n",
        "    running_loss = running_loss.detach().cpu().item()/len(train_loader)\n",
        "\n",
        "    # print(\"Epoch: \", epoch+1)\n",
        "    # print('running_loss', running_loss)\n",
        "\n",
        "    print(\"Epoch {}: Train Loss {:.08f}, Learning Rate {:.08f}\".format(\n",
        "        epoch + 1,\n",
        "        float(running_loss),\n",
        "        float(optimizer.param_groups[0]['lr'])))\n",
        "\n",
        "    # Optional: plot your attention for debugging\n",
        "    # plot_attention(attentions)\n",
        "\n",
        "    path = \"/content/drive/MyDrive/models/hw4p2/\"\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': running_loss,\n",
        "        }, path + \"hw4p2_model_v3_424_\" + str(epoch+1) + \".tar\" )\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JynGIYmFA5Ua"
      },
      "source": [
        "## Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVbfM5StA6-y"
      },
      "outputs": [],
      "source": [
        "def greedy_decode(probs):\n",
        "    # probs: FloatTensor (B, T, vocab_size)\n",
        "    out = []\n",
        "    for prob in probs:\n",
        "        s = []\n",
        "        for step in prob: #(T, vocab_size)\n",
        "            #             idx = torch.multinomial(step, 1)[0]\n",
        "            idx = step.argmax(0)\n",
        "            c = index2letter[idx]\n",
        "            if c == '<eos>':\n",
        "                break\n",
        "            s.append(c)\n",
        "        out.append(\"\".join(s))\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_FAsPmN55MZ"
      },
      "outputs": [],
      "source": [
        "def val(model, valid_loader):\n",
        "    model.eval()\n",
        "    total_dist = 0.0\n",
        "\n",
        "    batch_bar = tqdm(total=len(valid_loader), dynamic_ncols=True, leave=False, position=0, desc='Val★') \n",
        "\n",
        "    for batch_num, (x,y,x_len,y_len) in enumerate(valid_loader):\n",
        "        # 1) Send the inputs to the device\n",
        "        x, x_len, y, y_len = x.to(device), x_len.to(device), y.to(device), y_len.to(device)\n",
        "        with torch.no_grad():\n",
        "            # 2) Pass your inputs, and length of speech into the model.\n",
        "            predictions, attentions = model(x, x_len, y, mode='val') # (B, Tmax, vocab_size)\n",
        "                    \n",
        "        \n",
        "        # When computing Levenshtein distance, make sure you truncate prediction/target\n",
        "        # TODO: use beam search to decode!!!\n",
        "        #log_prob = F.softmax(input=predictions,dim=2) # need this for beam search\n",
        "        # print(log_prob[0][0].sum()) -- log = False\n",
        "        # print(predictions[0][0].sum()) -- log = True\n",
        "        # break\n",
        "\n",
        "        # use greedy_decode to generate pred_text\n",
        "        # pdb.set_trace()\n",
        "        # decoder1\n",
        "        # pred_text = transform_index_to_letter(predictions.argmax(-1).detach().cpu().numpy()) #(B,Tmax,1)\n",
        "        # decoder2\n",
        "        pred_text = greedy_decode(predictions.detach().cpu().numpy())\n",
        "        # decoder3 my beam decoder\n",
        "        #pred_text = decoder.decode(log_prob.detach().cpu().numpy())\n",
        "\n",
        "        target_text = transform_index_to_letter(y.detach().cpu().numpy())\n",
        "        \n",
        "        # if batch_num%10 == 1:\n",
        "        #     print(\"OG predication\")\n",
        "        #     print(pred_text[:1])\n",
        "\n",
        "        #  truncate prediction/target\n",
        "        #  y_len inclued <eos>\n",
        "        for i in range(len(y_len)):\n",
        "            # pred_text[i] = pred_text[i][:y_len[i]-1] # disable line up \n",
        "            target_text[i] = target_text[i][:y_len[i]-1]\n",
        "\n",
        "        # if batch_num%10 == 1:\n",
        "        #     print(pred_text[:1])\n",
        "        #     print(target_text[:1])\n",
        "\n",
        "        curr_dist = calc_edit_dist(pred_text, target_text) # averaged\n",
        "        total_dist += curr_dist \n",
        "        running_dist = total_dist / (batch_num + 1)\n",
        "        #print('curr_dist', running_dist)\n",
        "\n",
        "        # if  batch_num % 10 == 1: \n",
        "        #     print('curr_dist', curr_dist)\n",
        "        #     print('running_dist', running_dist)\n",
        "        batch_bar.set_postfix(\n",
        "            curr_dist=\"{:.04f}\".format(curr_dist),\n",
        "            running_dist=\"{:.04f}\".format(running_dist))\n",
        "        batch_bar.update()\n",
        "    batch_bar.close() \n",
        "    \n",
        "    print(\"Val: Distance {:.04f}\".format(running_dist))\n",
        "    return(running_dist)\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "wr-USN7ZKS9L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTwUwHUynXr2"
      },
      "outputs": [],
      "source": [
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    \n",
        "    pred_list = []\n",
        "    for batch_num, (x,x_len) in enumerate(test_loader):\n",
        "        # 1) Send the inputs to the device\n",
        "        x, x_len = x.to(device), x_len.to(device)\n",
        "        with torch.no_grad():\n",
        "            # 2) Pass your inputs, and length of speech into the model.\n",
        "            predictions, _ = model(x, x_len, mode='test') # (B, Tmax, vocab_size)\n",
        "\n",
        "        # TODO: use beam search to decode\n",
        "        # decoder1\n",
        "        # pred_text = transform_index_to_letter(predictions.argmax(-1).detach().cpu().numpy()) #(B,Tmax,1)\n",
        "        # decoder2\n",
        "        pred_text = greedy_decode(predictions.detach().cpu().numpy())\n",
        "        # decoder3\n",
        "        # pred_text = decoder.decode(predictions.detach().cpu().numpy())\n",
        "        # decoder4: ctcbeam\n",
        "        # pred_text,_,_,_ = decoder.decode(predictions)  #out = (B,k,T)\n",
        "        # pred_text,_,_,_ = decoder_test.decode(predictions)  #out = (B,k,T)\n",
        "        # pred_text = pred_text[:,0,:]\n",
        "        # pred_text = transform_index_to_letter(pred_text.detach().cpu().numpy())\n",
        "\n",
        "        for pred_text_one in pred_text:#one single transcript\n",
        "            pred_list.append(pred_text_one)\n",
        "    return pred_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYy3Lsi4VIrM"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZJ0yR9WU9wz"
      },
      "outputs": [],
      "source": [
        "# TODO: Define your model and put it on the device here\n",
        "# ...\n",
        "n_epochs = 150\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.00075000)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.75, patience=4, verbose=True, threshold=1e-2)\n",
        "# Make sure you understand the implication of setting reduction = 'none'\n",
        "criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "mode = 'train'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('just test')\n",
        "#path = \"/content/drive/MyDrive/models/hw4p2/hw4p2_model_v3_424_268.tar\"\n",
        "#checkpoint = torch.load(path)\n",
        "#model.load_state_dict(checkpoint['model_state_dict'])\n",
        "epoch0 = checkpoint['epoch']\n",
        "print(epoch0)\n",
        "#train(model, train_loader, criterion, optimizer, mode, epoch0+1)\n",
        "dist = val(model, val_loader)\n",
        "print(\"dist:\")\n",
        "print(dist)\n"
      ],
      "metadata": {
        "id": "vQpH1Hcxid_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3T7uGcBA3Se1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1f2c8eb-09d6-4184-d713-0e26790af856"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "just test\n",
            "274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Val★:   0%|          | 0/22 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "                                                                                            "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: Distance 6.8380\n",
            "dist:\n",
            "6.838044507575758\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "print('just test')\n",
        "path = \"/content/drive/MyDrive/models/hw4p2/hw4p2_model_v3_424_275.tar\"\n",
        "checkpoint = torch.load(path)\n",
        "#model.load_state_dict(checkpoint['model_state_dict'])\n",
        "epoch0 = checkpoint['epoch']\n",
        "print(epoch0)\n",
        "#train(model, train_loader, criterion, optimizer, mode, epoch0+1)\n",
        "dist = val(model, val_loader)\n",
        "print(\"dist:\")\n",
        "print(dist)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for what in checkpoint['model_state_dict']:\n",
        "#     print(what)"
      ],
      "metadata": {
        "id": "b9IMlywWbQ8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2O07xe4kdQ3i",
        "outputId": "3be47fee-f718-4b56-9908-e4a5ae8d38bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/models/hw4p2/hw4p2_model_v3_424_268.tar\n",
            "Epoch end at: 267\n"
          ]
        }
      ],
      "source": [
        "if CON_TRAIN:\n",
        "    path = '/content/drive/MyDrive/models/hw4p2/hw4p2_model_v3_424_268.tar'\n",
        "    print(path)\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.00013348388671875002)\n",
        "    #optimizer.load_state_dict(checkpoint['optimizer_state_dict']) # maybe not use this # cannot resume optimizer properly\n",
        "    epoch0 = checkpoint['epoch']\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.75, patience=5, verbose=True, threshold=1e-4)\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    \n",
        "    print('Epoch end at:', epoch0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El9pKayyzspq"
      },
      "source": [
        "# Predicting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "nG3MbxJ2WnPD",
        "outputId": "6c318986-a8c6-400a-9f6b-c681c191f6a7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-65f69eeb-3774-4707-875f-a4306a9e5971\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>predictions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>HE BEGAN A CONFUSED CONTLAIN TO GENS TO THE WI...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>KIH HAVE NOT SO EARNEST A MINE TO THESE MURMER...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>A GOLDEN FORTUNE AND A HAPPY LIFE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>HE WAS LIKE UP TO MY FATHER AND AWAY THAN YET ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>OSSO THERE WAS ASTRIPLING PAGE WHO TURNED ATTO...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-65f69eeb-3774-4707-875f-a4306a9e5971')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-65f69eeb-3774-4707-875f-a4306a9e5971 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-65f69eeb-3774-4707-875f-a4306a9e5971');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   id                                        predictions\n",
              "0   0  HE BEGAN A CONFUSED CONTLAIN TO GENS TO THE WI...\n",
              "1   1  KIH HAVE NOT SO EARNEST A MINE TO THESE MURMER...\n",
              "2   2                  A GOLDEN FORTUNE AND A HAPPY LIFE\n",
              "3   3  HE WAS LIKE UP TO MY FATHER AND AWAY THAN YET ...\n",
              "4   4  OSSO THERE WAS ASTRIPLING PAGE WHO TURNED ATTO..."
            ]
          },
          "execution_count": 285,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#df = pd.read_csv(\"hw4p2_416_submission_v2.csv\")\n",
        "# First 5 rows\n",
        "#df.head()\n",
        "# print(len(df)) # = 2620"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsCfWLXR4tYk",
        "outputId": "c942d43e-2a49-4d0f-f51d-81d08768475d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "pred_list = test(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq_nWR5741jw"
      },
      "source": [
        "##Write to csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3p-MY801XU_y",
        "outputId": "4f113a21-9ac7-47c8-8ef4-8a3bc0225244"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['HE BEGAN A CONFUSED COMPLAINT AGAINST THE WISHER WHO ADVANTAGE BEHIND THE CURTAIN ON THE LEFT',\n",
              " 'KIY HAVE NOT SO EARNEST A MIND TO THESE MUMMERIES CHILD',\n",
              " 'A GOLDEN FORTUNE AND A HAPPY LIFE',\n",
              " 'HE WAS LIKE UP TO MY FATHER IN A WAY AND YET WAS NOT MY FATHER',\n",
              " 'ALSO THERE WAS A STRIPLING PAGE WHO TURNED IT TO A MAID',\n",
              " 'THIS WAS SO SWEETLED EIGHTY SIR AND IN SOME MANNER I DO THINK SHE DIED',\n",
              " 'BUT THEN THE PICTURE WAS GONE AS QUICKLY AS IT CAME',\n",
              " 'SISTER NOW DO YOU HEAR THESE MARVELS',\n",
              " 'TAKE YOUR PLACE AND LET US SEE WITH THE CRIST BOTH AND SHOW TO YOU',\n",
              " 'LIKE AS NOT YOUNG MASTER THOUGH I AM AN OLD MAN']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "pred_list[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CL3wJg44434K"
      },
      "outputs": [],
      "source": [
        "# TODO: Generate the csv file\n",
        "with open(\"hw4p2_427_submission_v4.csv\", \"w+\") as f:\n",
        "    f.write(\"id,predictions\\n\")\n",
        "    for i in range(len(test_data)):\n",
        "        f.write(\"{},{}\\n\".format(i, pred_list[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "of7T_GXS5Qhq",
        "outputId": "88790bbb-73ea-415b-ea20-bab6bd9a18de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.8)\n",
            "100% 290k/290k [00:02<00:00, 121kB/s]\n",
            "Successfully submitted to Attention-Based Speech Recognition"
          ]
        }
      ],
      "source": [
        "# if SUBMIT_TO_KAGGLE:\n",
        "if 1:\n",
        "    !kaggle competitions submit -c 11-785-s22-hw4p2 -f hw4p2_427_submission_v4.csv -m \"v4 atp1\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "las.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
